## Using Apache Mahout for Machine Learning

### About This Lab

**Objective:** To run Mahout on a Hadoop cluster

**Successful outcome:** You will run a Mahout item-based recommender using the movielens dataset of movies

**File locations:** `~/labs`

---
### Steps

<!--STEP-->
<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>1 . View the Data</h2>

1\.  You should see four data files in this folder:

```
# ls –la 
total 24336 
-rw-r--r--	1 root root	171308		movies.dat
-rw-r--r--	1 root root	24594131	ratings.dat
-rw-r--r--	1 root root	5189		README
-rw-r--r--	1 root root	134368		users.dat
```

2\.  View the records in the `movies.dat` file using the tail command:

```
tail movies.dat
```

Output:

```
3943::Bamboozled (2000)::Comedy
3944::Bootmen (2000)::Comedy|Drama
3945::Digimon: The Movie (2000)::Adventure|Animation|Children's 
3946::Get Carter (2000)::Action|Drama|Thriller
3947::Get Carter (1971)::Thriller
3948::Meet the Parents (2000)::Comedy
3949::Requiem for a Dream (2000)::Drama
3950::Tigerland (2000)::Drama
3951::Two Family House (2000)::Drama
3952::Contender, The (2000)::Drama|Thriller
```

> Note  each record has an ID, a movie title, the year it was released, and the genres that it belongs to.

4\.  Similarly, view the contents of `ratings.dat`:

```
# tail ratings.dat 
6040::2022::5::956716207
6040::2028::5::956704519
6040::1080::4::957717322
6040::1089::4::956704996
6040::1090::3::956715518
6040::1091::1::956716541
6040::1094::5::956704887
6040::562::5::956704746
6040::1096::4::956715648
6040::1097::4::956715569
```

Each record is a user ID, movie ID, rating value, and timestamp. Plus, notice the data is separated by colons.


<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>2. Put the Data in HDFS</h2>

1\.  Create a new folder in HDFS named rec:

```
hdfs dfs -mkdir rec
```

2\.  Put ratings.dat into the rec folder:

```
hdfs dfs -put ratings.dat rec/
```

3\.  Verify the file is in the `rec` folder:

```
# hdfs dfs -ls rec 
Found 1 items
-rw-r--r--	3 root root	24594131	rec/ratings.dat
```


<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>3. Transform the Data</h2>

1\. The `ratings.dat` data is colon-separated, but the Mahout function we are going to use requires comma-separated data. In this step, you will replace the colons with commas using a script that has been provided for you. The script will also remove the timestamp, which is not needed.

Start by changing directories to the folder that contains the scripts

2\.  View the contents of the script named `parse.awk`:

```
more parse.awk 
```

Output:

```
!/bin/awk –f

BEGIN { FS = "::" }
{ print $1","$2","$3 }
```

> Note  the print command prints the first three records, and prints a comma between each record.

3\. Run the following command (which has to be entered on a single line), which executes a MapReduce job with parse.awk as the mapper and outputs the result into a folder named `rec/ratings_csv` in HDFS:

```
yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -files parse.awk -input rec/ratings.dat -output rec/ratings_csv -mapper parse.awk
```

4\. When the job finishes, verify you have a new folder in HDFS named `rec/ratings_csv`:

```
hdfs dfs -ls -R rec/
-rw-r--r--	3 root root	24594131 rec/ratings.dat 
drwxr-xr-x	- root root	       0 rec/ratings_csv
-rw-r--r--	3 root root	       0 rec/ratings_csv/_SUCCESS
-rw-r--r--	3 root root	12553665 rec/ratings_csv/part-00000
```

5\.  View the contents of the output file generated by the MapReduce job:

```
hdfs dfs -tail rec/ratings_csv/part-00000
```

The end of the file should look like:
```
999,9,4 
999,912,3 
999,919,5 
999,923,2 
999,924,2 
999,95,2 
999,996,2
```

> Note  the colons are replaced with commas and the timestamp is removed

---

> NOTE: The above steps illustrate the use of MapReduce and scripting to prepare our data. Data preparation, including field interpretation re-formatting and aggregation (i.e., data munging), null removals and delimiter replacement (i.e., data cleansing), can often be the most time consuming portion of the data science process. Later in the class, we will learn how to use a powerful data transformation tool called Pig to further help us with data preparation.



<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>4. Run the Mahout Recommender</h2>

1\.  Enter the mahout command without any arguments:

```
mahout
```

You should see all of the algorithms and other functions available in Mahout.

2\. To view the usage of a Mahout algorithm, enter the name of the algorithm. For example, type in the following to view the usage of `recommenditembased`:

```
mahout recommenditembased
```
> Note  at a minimum you must specify a similarity measures class.

3\. Enter the following command (all on a single line), which runs the recommenditembased algorithm on the data in the `ratings_csv` folder:

```
mahout recommenditembased -i rec/ratings_csv -o rec/output --tempDir rec/temp -s SIMILARITY_COSINE
```

4\. Wait for the job to complete, which may take 5-10 minutes. It takes nine different MapReduce jobs to compute the result. The job is finished when you see the following output:
    
```
File Input Format Counters 
        Bytes Read=8302150
File Output Format Counters 
        Bytes Written=588320
INFO driver.MahoutDriver: Program took 336728 ms (Minutes: 5.612133333333333)
```


<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>5. View the Results</h2>

1\.  View the contents of `rec/output`:

```
hdfs dfs -ls rec/output
```
Output:
```
Found 2 items
-rw-r--r-- 3 root root 0 rec/output/_SUCCESS 
-rw-r--r-- 3 root root 588320 rec/output/part-r-00000
```

You should see a `part-r-00000` file.

2\.  View the contents of the output file:

```
hdfs dfs -tail rec/output/part-r-00000
```
Output:
```
6036[3366:5.0,1407:5.0,1301:5.0,2583:5.0,2966:5.0,3198:5.0,3362:5.0,3635:5.0, 3173:5.0,1894:5.0] 6037[45:5.0,1407:5.0,1958:5.0,1805:5.0,2501:5.0,3507:5.0,914:5.0,3148:5.0,326 0:5.0,2735:5.0] 6038[3429:5.0,1277:5.0,1278:5.0,39:5.0,3000:5.0,235:5.0,2700:5.0,2857:5.0,190 7:5.0,2761:5.0] 6039[3341:5.0,965:5.0,2857:5.0,2303:5.0,47:5.0,1281:5.0,1078:5.0,3421:5.0,194 6:5.0,1676:5.0] 6040[2966:5.0,3507:5.0,3148:5.0,1827:5.0,45:5.0,495:5.0,914:5.0,3007:5.0,1958 :5.0,1407:5.0]
```

> Note  each user has 10 recommended movies, along with a predicted score for each recommendation.



<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>6. Get the Results from HDFS</h2>

1\. Change to the following directory:

```
cd ~/labs/recsys-fe/
```

2\. There may be a file in this folder named `rec_file.txt` that is used by the Web app. You need to delete it:

```
rm rec_file.txt
```

3\. Retrieve the results file from HDFS and copy it onto the local filesystem using the following command:

```
hdfs dfs -get rec/output/part-r-00000 rec_file.txt
```


<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>7. View the Results in a Web App</h2>

1\.  Start the Web app using the following command:

```
ruby server.rb
```

> Note  This terminal is not usable for a few steps while Ruby runs. (Open another terminal if you need a command prompt.)

2\.  Point your Web browser to the following URL:

```
http://<your IP Address>:4567
```

You should see the following page:

![using-apache-mahout-for-machine-learning-1](https://user-images.githubusercontent.com/21102559/40943042-d9c9542a-681d-11e8-8535-9d4560a5f89d.png)

3\.  Enter a user ID to view the movies recommended for that user. For example, try user `55`:

![using-apache-mahout-for-machine-learning-2](https://user-images.githubusercontent.com/21102559/40943043-d9dd0cf4-681d-11e8-8e09-0b7e10fef882.png)

> Note  Your results may vary because there is an element of randomization to the algorithm. There may be more than 10 recommended movies, so you might get a different 10 movies each time you run the algorithm.

4\.  At the command prompt, enter `Ctrl+c` to kill the Web application.



<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>8 . Run the Recommender Again</h2>

1\. This time, you are going to use the co-occurrence similarity. But first, you need to clean up some folders in HDFS. Start by deleteing the rec/temp folder from HDFS:

```
hdfs dfs -rm -R -skipTrash rec/temp
```

2\.  Remove the `rec/output` folder also:

```
hdfs dfs -rm -R -skipTrash rec/output
```

3\.  Run the same Mahout command as before (all on a single line), except this time use the `SIMILARITY_COOCCURRENCE` class:

```
mahout recommenditembased -i rec/ratings_csv -o rec/output --tempDir rec/temp -s SIMILARITY_COOCCURRENCE 
```

4\. Wait for the job to execute, which may take 5-10 minutes. 


<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>9. View the Results</h2>

1\. Make a backup of the local copy of the `COSINE` results. The following commands assume you are in the `~labs` folder:

```
mv rec_file.txt cos_results.txt
```

2\.  Get the co-occurrence results from HDFS:

```
hdfs dfs -get rec/output/part-r-00000 rec_file.txt
```

3\.  Run the Web app again:

```
ruby server.rb
```

4\.  Go back to your Web browser and in a new Firefox tab view the movie recommendations for the same user.

> Note  the two similarity classes result in different movies being recommended for the same user. For example, in my case the co-occurrence method chose the following movies for user 55:

![using-apache-mahout-for-machine-learning-3](https://user-images.githubusercontent.com/21102559/40943044-d9ec001a-681d-11e8-92da-c954320c88c9.png)



<!--STEP-->

<img src="https://user-images.githubusercontent.com/558905/40613898-7a6c70d6-624e-11e8-9178-7bde851ac7bd.png" align="left" width="50" height="50" title="ToDo Logo" />
<h2>10. Clean Up the Folders</h2>

1\.  Delete the `rec/temp` and `rec/output` folders from HDFS:

```
hdfs dfs -rm -R -skipTrash rec/temp
hdfs dfs -rm -R -skipTrash rec/output 
```

### Result

You have executed a machine learning recommender engine on a Hadoop cluster using Mahout's item recommender with both the cosine and co-occurrence similarity classes.

You are finished!
